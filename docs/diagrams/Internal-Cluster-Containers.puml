@startuml Internal Cluster Containers
!includeurl https://raw.githubusercontent.com/plantuml-stdlib/C4-PlantUML/master/C4_Container.puml
LAYOUT_WITH_LEGEND()
HIDE_STEREOTYPE()

Person(dev, "Developer", "Runs Terraform and Helmfile deployments")

System_Ext(keyvault, "Azure Key Vault", "KMS for SOPS encryption keys")
System_Ext(registry, "Container Registry", "Stores service images")

System_Boundary(tooling, "Deployment Tooling") {
    Container(helmfile, "Helmfile + SOPS", "CLI", "Applies Helm charts and decrypts secrets")
}

System_Boundary(home, "Home Lab") {
    Container(kvmhost, "KVM/libvirt Host", "Ubuntu + libvirt", "Runs the VM fleet")

    Container_Boundary(cluster, "Internal Kubernetes Cluster") {
        Container(cilium, "Cilium", "CNI", "Pod networking and service routing")
        Container(localpath, "local-path-provisioner", "StorageClass", "Default local persistent volumes")
        Container(certmanager, "cert-manager", "Controller", "Issues TLS certificates for ingress hosts")
        Container(ingress, "ingress-nginx", "Ingress", "HTTP/TCP routing for cluster services")
        Container(ingressResources, "Ingress resources", "Manifests", "Ingress routes, TCP mappings, and local CA certificates")
        Container(externalSecrets, "External Secrets Operator", "Controller", "Syncs Key Vault secrets into Kubernetes")
        Container(externalSecretsResources, "External Secrets resources", "Manifests", "SecretStore and ExternalSecret definitions")
        Container(postgres, "PostgreSQL", "Database", "Metadata and operational databases")
        Container(minio, "MinIO", "Object storage", "S3-compatible buckets")
        Container(airflow, "Airflow", "Workflow engine", "Schedules and executes pipelines")
        Container(spark, "Spark Operator", "Controller", "Runs SparkApplication workloads")
        Container(airflowSparkRbac, "Airflow Spark RBAC", "RBAC", "Grants Airflow workers SparkApplication permissions")
        Container(ivyCache, "Ivy cache", "PVC", "Shared JAR cache for Spark driver/executor pods")
        Container(prometheus, "Prometheus", "Metrics", "Collects cluster metrics")
        Container(grafana, "Grafana", "Dashboards", "Visualizes metrics")
        Container(keda, "KEDA", "Autoscaler", "Scales Airflow workers")
    }
}

Rel(dev, helmfile, "Runs deployment commands")
Rel(helmfile, cluster, "Applies releases", "Helm")
Rel(helmfile, keyvault, "Decrypts secrets", "SOPS")
Rel(externalSecrets, keyvault, "Syncs secrets")
Rel(externalSecretsResources, externalSecrets, "Defines SecretStore/ExternalSecret")
Rel(certmanager, ingress, "Issues TLS certificates")
Rel(ingressResources, ingress, "Defines routes and TCP mappings")
Rel(ingress, airflow, "Routes HTTP")
Rel(ingress, minio, "Routes HTTP/S3")
Rel(ingress, postgres, "Routes TCP", "5432")
Rel(ingress, grafana, "Routes HTTP")
Rel(cluster, registry, "Pulls images", "Registry")
Rel(airflow, postgres, "Stores metadata", "SQL")
Rel(airflow, minio, "Reads/writes objects", "S3")
Rel(airflow, spark, "Submits jobs", "SparkApplication")
Rel(airflowSparkRbac, airflow, "Grants SparkApplication access", "RBAC")
Rel(spark, postgres, "Reads data", "SQL")
Rel(spark, minio, "Writes data", "S3")
Rel(spark, ivyCache, "Reads JARs", "PVC")
Rel(prometheus, cluster, "Scrapes metrics")
Rel(grafana, prometheus, "Queries metrics")
Rel(keda, airflow, "Scales workers", "KEDA API")

SHOW_LEGEND()
@enduml
